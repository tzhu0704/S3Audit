#!/usr/bin/env python3
"""
S3 æ•°æ®ä¸¢å¤±åˆ†æå·¥å…·
åˆ†æ S3 bucket çš„å†å²æ•°æ®,æ£€æµ‹å¯èƒ½çš„æ•°æ®ä¸¢å¤±
"""

import boto3
import argparse
import json
import os
from datetime import datetime, timedelta
from collections import defaultdict

class S3DeletionAnalyzer:
    def __init__(self, bucket_name, region='us-east-1', skip_object_listing=False):
        self.bucket_name = bucket_name
        self.region = region
        self.skip_object_listing = skip_object_listing
        self.s3_client = boto3.client('s3', region_name=region)
        self.cloudwatch = boto3.client('cloudwatch', region_name=region)
        self.cloudtrail = boto3.client('cloudtrail', region_name=region)
        self.ce_client = boto3.client('ce', region_name='us-east-1')  # Cost Explorer åªåœ¨ us-east-1
        self.findings = []
        
        # åˆ›å»º logs ç›®å½•(åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.logs_dir = os.path.join(script_dir, 'logs')
        os.makedirs(self.logs_dir, exist_ok=True)
        
    def analyze(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print(f"\n{'='*80}")
        print(f"S3 æ•°æ®ä¸¢å¤±åˆ†ææŠ¥å‘Š")
        print(f"Bucket: {self.bucket_name}")
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}\n")
        
        # 1. CloudWatch æŒ‡æ ‡åˆ†æ
        print("[1/7] åˆ†æ CloudWatch å†å²æŒ‡æ ‡...")
        self._analyze_cloudwatch_metrics()
        
        # 2. ç‰ˆæœ¬æ§åˆ¶æ£€æŸ¥
        print("[2/7] æ£€æŸ¥ç‰ˆæœ¬æ§åˆ¶å’Œåˆ é™¤æ ‡è®°...")
        self._check_versioning()
        
        # 3. ç”Ÿå‘½å‘¨æœŸç­–ç•¥æ£€æŸ¥
        print("[3/7] æ£€æŸ¥ç”Ÿå‘½å‘¨æœŸç­–ç•¥...")
        self._check_lifecycle_policy()
        
        # 4. CloudTrail äº‹ä»¶æ£€æŸ¥
        print("[4/7] æ£€æŸ¥ CloudTrail ç®¡ç†äº‹ä»¶...")
        self._check_cloudtrail_events()
        
        # 5. Bucket ç­–ç•¥æ£€æŸ¥
        print("[5/7] æ£€æŸ¥ Bucket ç­–ç•¥...")
        self._check_bucket_policy()
        
        # 6. æˆæœ¬åˆ†æ
        print("[6/7] åˆ†æ S3 æˆæœ¬å˜åŒ–...")
        self._analyze_costs()
        
        # 7. å½“å‰å¯¹è±¡ç»Ÿè®¡
        if not self.skip_object_listing:
            print("[7/7] ç»Ÿè®¡å½“å‰å¯¹è±¡...")
            self._analyze_current_objects()
        else:
            print("[7/7] è·³è¿‡å¯¹è±¡ç»Ÿè®¡(ä½¿ç”¨ --skip-listing å‚æ•°)...")
            self.current_stats = {'skipped': True}
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\nç”Ÿæˆåˆ†ææŠ¥å‘Š...\n")
        self._generate_report()
        
    def _analyze_cloudwatch_metrics(self):
        """åˆ†æ CloudWatch æŒ‡æ ‡"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=90)
        
        try:
            # è·å–å­˜å‚¨é‡æŒ‡æ ‡
            size_response = self.cloudwatch.get_metric_statistics(
                Namespace='AWS/S3',
                MetricName='BucketSizeBytes',
                Dimensions=[
                    {'Name': 'BucketName', 'Value': self.bucket_name},
                    {'Name': 'StorageType', 'Value': 'StandardStorage'}
                ],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Average']
            )
            
            # è·å–å¯¹è±¡æ•°é‡æŒ‡æ ‡
            count_response = self.cloudwatch.get_metric_statistics(
                Namespace='AWS/S3',
                MetricName='NumberOfObjects',
                Dimensions=[
                    {'Name': 'BucketName', 'Value': self.bucket_name},
                    {'Name': 'StorageType', 'Value': 'AllStorageTypes'}
                ],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Average']
            )
            
            # åˆ†æå­˜å‚¨é‡å˜åŒ–
            size_data = sorted(size_response['Datapoints'], key=lambda x: x['Timestamp'])
            count_data = sorted(count_response['Datapoints'], key=lambda x: x['Timestamp'])
            
            if len(size_data) > 1:
                size_changes = []
                for i in range(1, len(size_data)):
                    prev_size = size_data[i-1]['Average']
                    curr_size = size_data[i]['Average']
                    change = curr_size - prev_size
                    change_pct = (change / prev_size * 100) if prev_size > 0 else 0
                    
                    if change_pct < -10:  # ä¸‹é™è¶…è¿‡ 10%
                        size_changes.append({
                            'date': size_data[i]['Timestamp'].strftime('%Y-%m-%d'),
                            'change_gb': change / (1024**3),
                            'change_pct': change_pct,
                            'prev_size_gb': prev_size / (1024**3),
                            'curr_size_gb': curr_size / (1024**3)
                        })
                
                if size_changes:
                    self.findings.append({
                        'severity': 'HIGH',
                        'category': 'CloudWatch æŒ‡æ ‡å¼‚å¸¸',
                        'title': f'æ£€æµ‹åˆ° {len(size_changes)} æ¬¡æ˜¾è‘—çš„å­˜å‚¨é‡ä¸‹é™',
                        'details': size_changes
                    })
            
            # åˆ†æå¯¹è±¡æ•°é‡å˜åŒ–
            if len(count_data) > 1:
                count_changes = []
                for i in range(1, len(count_data)):
                    prev_count = count_data[i-1]['Average']
                    curr_count = count_data[i]['Average']
                    change = curr_count - prev_count
                    
                    if change < -100:  # å‡å°‘è¶…è¿‡ 100 ä¸ªå¯¹è±¡
                        count_changes.append({
                            'date': count_data[i]['Timestamp'].strftime('%Y-%m-%d'),
                            'objects_deleted': int(abs(change)),
                            'prev_count': int(prev_count),
                            'curr_count': int(curr_count)
                        })
                
                if count_changes:
                    self.findings.append({
                        'severity': 'HIGH',
                        'category': 'CloudWatch æŒ‡æ ‡å¼‚å¸¸',
                        'title': f'æ£€æµ‹åˆ° {len(count_changes)} æ¬¡æ˜¾è‘—çš„å¯¹è±¡æ•°é‡å‡å°‘',
                        'details': count_changes
                    })
            
            # ä¿å­˜åŸå§‹æ•°æ®ä¾›æŠ¥å‘Šä½¿ç”¨
            self.size_data = size_data
            self.count_data = count_data
            
        except Exception as e:
            self.findings.append({
                'severity': 'INFO',
                'category': 'CloudWatch æŒ‡æ ‡',
                'title': 'æ— æ³•è·å– CloudWatch æŒ‡æ ‡',
                'details': str(e)
            })
    
    def _check_versioning(self):
        """æ£€æŸ¥ç‰ˆæœ¬æ§åˆ¶å’Œåˆ é™¤æ ‡è®°ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰"""
        try:
            versioning = self.s3_client.get_bucket_versioning(Bucket=self.bucket_name)
            status = versioning.get('Status', 'Disabled')
            
            if status == 'Enabled':
                print("  ç‰ˆæœ¬æ§åˆ¶å·²å¯ç”¨ï¼Œå¼€å§‹åˆ†æç‰ˆæœ¬æ•°æ®...")
                # æµå¼å¤„ç†ç‰ˆæœ¬æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º
                try:
                    # è®¡ç®—3ä¸ªæœˆå‰çš„æ—¶é—´
                    three_months_ago = datetime.utcnow() - timedelta(days=90)
                    
                    delete_markers = []
                    noncurrent_by_key = {}
                    total_versions = 0
                    noncurrent_count = 0
                    total_delete_markers_count = 0  # æ€»åˆ é™¤æ ‡è®°æ•°ï¼ˆåŒ…æ‹¬æœªä¿å­˜çš„ï¼‰
                    
                    # ä¸¥æ ¼é™åˆ¶å†…å­˜ä½¿ç”¨
                    MAX_DELETE_MARKERS = 10000  # é™ä½åˆ°10000
                    MAX_NONCURRENT_KEYS = 10000  # é™ä½åˆ°10000
                    
                    # ä½¿ç”¨åˆ†é¡µå™¨æµå¼å¤„ç†
                    paginator = self.s3_client.get_paginator('list_object_versions')
                    page_iterator = paginator.paginate(
                        Bucket=self.bucket_name,
                        PaginationConfig={'PageSize': 1000}
                    )
                    
                    page_num = 0
                    processing_failed = False
                    error_message = None
                    
                    for page in page_iterator:
                        page_num += 1
                        if page_num % 10 == 0:
                            print(f"  å·²å¤„ç† {page_num} é¡µç‰ˆæœ¬æ•°æ® (åˆ é™¤æ ‡è®°: {total_delete_markers_count}, éå½“å‰ç‰ˆæœ¬: {noncurrent_count})...")
                        
                        # å¤„ç†åˆ é™¤æ ‡è®°ï¼ˆåªä¿ç•™æœ€è¿‘3ä¸ªæœˆçš„ï¼Œä¸”ä¸¥æ ¼é™åˆ¶æ•°é‡ï¼‰
                        for dm in page.get('DeleteMarkers', []):
                            if dm.get('IsLatest', False) and dm['LastModified'].replace(tzinfo=None) >= three_months_ago:
                                total_delete_markers_count += 1
                                
                                # åªä¿å­˜å‰MAX_DELETE_MARKERSä¸ªæœ€æ–°çš„åˆ é™¤æ ‡è®°
                                if len(delete_markers) < MAX_DELETE_MARKERS:
                                    delete_markers.append(dm)
                                elif dm['LastModified'] > delete_markers[-1]['LastModified']:
                                    # å¦‚æœæ–°çš„åˆ é™¤æ ‡è®°æ›´æ–°ï¼Œæ›¿æ¢æœ€æ—§çš„
                                    delete_markers[-1] = dm
                                    # æ¯1000ä¸ªå°±é‡æ–°æ’åºä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹æ’åº
                                    if total_delete_markers_count % 1000 == 0:
                                        delete_markers.sort(key=lambda x: x['LastModified'], reverse=True)
                        
                        # å¤„ç†ç‰ˆæœ¬ï¼ˆåªç»Ÿè®¡æœ€è¿‘3ä¸ªæœˆçš„éå½“å‰ç‰ˆæœ¬ï¼‰
                        for v in page.get('Versions', []):
                            if v['LastModified'].replace(tzinfo=None) >= three_months_ago:
                                total_versions += 1
                                
                                if not v.get('IsLatest', False):
                                    noncurrent_count += 1
                                    key = v['Key']
                                    
                                    # åªè·Ÿè¸ªå‰MAX_NONCURRENT_KEYSä¸ªå¯¹è±¡
                                    if len(noncurrent_by_key) < MAX_NONCURRENT_KEYS:
                                        if key not in noncurrent_by_key:
                                            noncurrent_by_key[key] = {
                                                'count': 0,
                                                'latest_modified': v['LastModified'],
                                                'total_size': 0
                                            }
                                        
                                        noncurrent_by_key[key]['count'] += 1
                                        noncurrent_by_key[key]['total_size'] += v['Size']
                                        
                                        if v['LastModified'] > noncurrent_by_key[key]['latest_modified']:
                                            noncurrent_by_key[key]['latest_modified'] = v['LastModified']
                    
                    print(f"  ç‰ˆæœ¬æ•°æ®å¤„ç†å®Œæˆ: æ€»ç‰ˆæœ¬={total_versions}, åˆ é™¤æ ‡è®°={total_delete_markers_count}, éå½“å‰ç‰ˆæœ¬={noncurrent_count}")
                    
                    # æœ€ç»ˆæ’åºï¼ˆæ•°é‡å·²ç»è¢«é™åˆ¶ï¼‰
                    delete_markers.sort(key=lambda x: x['LastModified'], reverse=True)
                    
                    # æ„å»ºéå½“å‰ç‰ˆæœ¬åˆ†æ
                    noncurrent_analysis = []
                    for key, data in noncurrent_by_key.items():
                        noncurrent_analysis.append({
                            'key': key,
                            'noncurrent_count': data['count'],
                            'latest_noncurrent': data['latest_modified'].strftime('%Y-%m-%d %H:%M:%S'),
                            'total_size': data['total_size']
                        })
                    
                    # æŒ‰æœ€è¿‘ä¿®æ”¹æ—¶é—´æ’åº
                    noncurrent_analysis.sort(key=lambda x: x['latest_noncurrent'], reverse=True)
                    
                    version_info = {
                        'status': 'Enabled',
                        'total_versions': total_versions,
                        'noncurrent_versions': noncurrent_count,
                        'delete_markers': total_delete_markers_count,  # ä½¿ç”¨å®é™…æ€»æ•°
                        'objects_with_noncurrent': len(noncurrent_analysis)
                    }
                    
                    # ä¿å­˜ä¾›æŠ¥å‘Šä½¿ç”¨ï¼ˆå·²ç»è¢«é™åˆ¶æ•°é‡ï¼‰
                    self.version_analysis = {
                        'delete_markers': [{
                            'key': dm['Key'],
                            'last_modified': dm['LastModified'].strftime('%Y-%m-%d %H:%M:%S'),
                            'version_id': dm['VersionId']
                        } for dm in delete_markers],  # å·²ç»é™åˆ¶åœ¨MAX_DELETE_MARKERSå†…
                        'noncurrent_analysis': noncurrent_analysis,  # å·²ç»é™åˆ¶åœ¨MAX_NONCURRENT_KEYSå†…
                        'total_delete_markers': total_delete_markers_count,
                        'total_noncurrent_objects': len(noncurrent_analysis),
                        'time_range': f'æœ€è¿‘90å¤© ({three_months_ago.strftime("%Y-%m-%d")} è‡³ {datetime.utcnow().strftime("%Y-%m-%d")})'
                    }
                    
                    if total_delete_markers_count > 0:
                        title_msg = f'æœ€è¿‘90å¤©å‘ç° {total_delete_markers_count} ä¸ªåˆ é™¤æ ‡è®°'
                        if total_delete_markers_count > len(delete_markers):
                            title_msg += f' (æŠ¥å‘Šä¸­æ˜¾ç¤ºå‰ {len(delete_markers)} ä¸ª)'
                        
                        self.findings.append({
                            'severity': 'MEDIUM',
                            'category': 'ç‰ˆæœ¬æ§åˆ¶',
                            'title': title_msg,
                            'details': {
                                'message': f'è¿™äº›å¯¹è±¡åœ¨æœ€è¿‘90å¤©å†…è¢«æ ‡è®°ä¸ºåˆ é™¤,ä½†å¯ä»¥æ¢å¤',
                                'time_range': f'{three_months_ago.strftime("%Y-%m-%d")} è‡³ {datetime.utcnow().strftime("%Y-%m-%d")}',
                                'version_info': version_info
                            }
                        })
                    
                    if len(noncurrent_analysis) > 0:
                        self.findings.append({
                            'severity': 'INFO',
                            'category': 'ç‰ˆæœ¬æ§åˆ¶',
                            'title': f'æœ€è¿‘90å¤©å‘ç° {len(noncurrent_analysis)} ä¸ªå¯¹è±¡æœ‰éå½“å‰ç‰ˆæœ¬',
                            'details': {
                                'message': f'æœ€è¿‘90å¤©å†…å…± {noncurrent_count} ä¸ªéå½“å‰ç‰ˆæœ¬,å¯èƒ½åŒ…å«è¢«è¦†ç›–æˆ–åˆ é™¤çš„æ•°æ®',
                                'time_range': f'{three_months_ago.strftime("%Y-%m-%d")} è‡³ {datetime.utcnow().strftime("%Y-%m-%d")}',
                                'version_info': version_info
                            }
                        })
                    else:
                        self.findings.append({
                            'severity': 'INFO',
                            'category': 'ç‰ˆæœ¬æ§åˆ¶',
                            'title': 'ç‰ˆæœ¬æ§åˆ¶å·²å¯ç”¨',
                            'details': version_info
                        })
                except Exception as e:
                    processing_failed = True
                    error_message = str(e)
                    print(f"\n  âš ï¸  ç‰ˆæœ¬æ•°æ®å¤„ç†å¤±è´¥: {error_message}")
                    print(f"  å·²ä¿ç•™ {len(delete_markers)} ä¸ªåˆ é™¤æ ‡è®°çš„è¯¦ç»†ä¿¡æ¯")
                    print(f"  å°è¯•ç»§ç»­ç»Ÿè®¡å‰©ä½™åˆ é™¤æ ‡è®°æ€»æ•°...")
                    
                    # ç»§ç»­ç»Ÿè®¡å‰©ä½™çš„åˆ é™¤æ ‡è®°æ•°é‡ï¼ˆä¸ä¿å­˜è¯¦ç»†ä¿¡æ¯ï¼‰
                    remaining_dm_count = 0
                    try:
                        # ä»å½“å‰ä½ç½®ç»§ç»­æ‰«æ
                        for page in page_iterator:
                            remaining_dm_count += len([dm for dm in page.get('DeleteMarkers', []) if dm.get('IsLatest', False)])
                            if remaining_dm_count % 1000 == 0:
                                print(f"    ç»§ç»­æ‰«æ... å·²å‘ç°é¢å¤– {remaining_dm_count} ä¸ªåˆ é™¤æ ‡è®°")
                    except:
                        pass
                    
                    total_delete_markers_count += remaining_dm_count
                    print(f"  ç»Ÿè®¡å®Œæˆ: æ€»åˆ é™¤æ ‡è®°={total_delete_markers_count} (è¯¦ç»†ä¿¡æ¯={len(delete_markers)})")
                
                # å¤„ç†ç»“æœï¼ˆæ— è®ºæ˜¯å¦å‡ºé”™ï¼‰
                if processing_failed:
                    # ä¿å­˜å·²æ”¶é›†çš„éƒ¨åˆ†æ•°æ®
                    if delete_markers:
                        delete_markers.sort(key=lambda x: x['LastModified'], reverse=True)
                        self.version_analysis = {
                            'delete_markers': [{
                                'key': dm['Key'],
                                'last_modified': dm['LastModified'].strftime('%Y-%m-%d %H:%M:%S'),
                                'version_id': dm['VersionId']
                            } for dm in delete_markers],
                            'noncurrent_analysis': [],
                            'total_delete_markers': total_delete_markers_count,
                            'total_noncurrent_objects': 0,
                            'time_range': f'æœ€è¿‘90å¤© ({three_months_ago.strftime("%Y-%m-%d")} è‡³ {datetime.utcnow().strftime("%Y-%m-%d")})'
                        }
                        
                        self.findings.append({
                            'severity': 'HIGH',
                            'category': 'ç‰ˆæœ¬æ§åˆ¶',
                            'title': f'å‘ç° {total_delete_markers_count} ä¸ªåˆ é™¤æ ‡è®°ï¼ˆéƒ¨åˆ†è¯¦ç»†ä¿¡æ¯å¯ç”¨ï¼‰',
                            'details': {
                                'message': f'ç‰ˆæœ¬æ•°æ®å¤„ç†æ—¶å‡ºé”™ï¼Œä½†å·²ä¿å­˜ {len(delete_markers)} ä¸ªåˆ é™¤æ ‡è®°çš„è¯¦ç»†ä¿¡æ¯',
                                'total_delete_markers': total_delete_markers_count,
                                'detailed_info_available': len(delete_markers),
                                'error': error_message[:200],
                                'note': f'æŠ¥å‘Šä¸­æ˜¾ç¤ºå‰ {len(delete_markers)} ä¸ªåˆ é™¤æ ‡è®°çš„è¯¦ç»†ä¿¡æ¯',
                                'suggestion': 'å»ºè®®: 1) åœ¨å†…å­˜æ›´å¤§çš„æœºå™¨ä¸Šé‡æ–°è¿è¡Œä»¥è·å–å®Œæ•´ä¿¡æ¯ 2) ä½¿ç”¨ S3 Inventory è¿›è¡Œç¦»çº¿åˆ†æ 3) é…ç½®ç”Ÿå‘½å‘¨æœŸç­–ç•¥æ¸…ç†åˆ é™¤æ ‡è®°'
                            }
                        })
                    else:
                        self.findings.append({
                            'severity': 'MEDIUM',
                            'category': 'ç‰ˆæœ¬æ§åˆ¶',
                            'title': 'ç‰ˆæœ¬æ§åˆ¶å·²å¯ç”¨ï¼Œä½†æ— æ³•å®Œæ•´åˆ†æç‰ˆæœ¬æ•°æ®',
                            'details': {
                                'message': f'å¤„ç†ç‰ˆæœ¬æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼Œæ€»åˆ é™¤æ ‡è®°æ•°: {total_delete_markers_count}',
                                'total_delete_markers': total_delete_markers_count,
                                'error': error_message[:200],
                                'suggestion': 'å»ºè®®: 1) åœ¨å†…å­˜æ›´å¤§çš„æœºå™¨ä¸Šè¿è¡Œ 2) ä½¿ç”¨ S3 Inventory è¿›è¡Œç¦»çº¿åˆ†æ'
                            }
                        })
                    return  # æå‰è¿”å›ï¼Œä¸æ‰§è¡Œåç»­çš„æ­£å¸¸å¤„ç†é€»è¾‘
            else:
                self.findings.append({
                    'severity': 'INFO',
                    'category': 'ç‰ˆæœ¬æ§åˆ¶',
                    'title': 'ç‰ˆæœ¬æ§åˆ¶æœªå¯ç”¨',
                    'details': 'æ— æ³•è¿½è¸ªåˆ é™¤å†å²,å»ºè®®å¯ç”¨ç‰ˆæœ¬æ§åˆ¶'
                })
                
        except Exception as e:
            # è·å–ç‰ˆæœ¬æ§åˆ¶çŠ¶æ€å¤±è´¥
            if 'AccessDenied' not in str(e):
                print(f"  âš ï¸  æ— æ³•è·å–ç‰ˆæœ¬æ§åˆ¶çŠ¶æ€: {str(e)}")
                self.findings.append({
                    'severity': 'INFO',
                    'category': 'ç‰ˆæœ¬æ§åˆ¶',
                    'title': 'æ— æ³•è·å–ç‰ˆæœ¬æ§åˆ¶çŠ¶æ€',
                    'details': str(e)
                })
    
    def _check_lifecycle_policy(self):
        """æ£€æŸ¥ç”Ÿå‘½å‘¨æœŸç­–ç•¥"""
        try:
            response = self.s3_client.get_bucket_lifecycle_configuration(
                Bucket=self.bucket_name
            )
            
            rules = response.get('Rules', [])
            active_rules = [r for r in rules if r.get('Status') == 'Enabled']
            
            if active_rules:
                deletion_rules = []
                for rule in active_rules:
                    if 'Expiration' in rule or 'NoncurrentVersionExpiration' in rule:
                        deletion_rules.append({
                            'id': rule.get('ID', 'N/A'),
                            'expiration': rule.get('Expiration', {}),
                            'filter': rule.get('Filter', {})
                        })
                
                if deletion_rules:
                    self.findings.append({
                        'severity': 'HIGH',
                        'category': 'ç”Ÿå‘½å‘¨æœŸç­–ç•¥',
                        'title': f'å‘ç° {len(deletion_rules)} æ¡è‡ªåŠ¨åˆ é™¤è§„åˆ™',
                        'details': {
                            'message': 'è¿™äº›è§„åˆ™å¯èƒ½å¯¼è‡´æ•°æ®è‡ªåŠ¨åˆ é™¤',
                            'rules': deletion_rules
                        }
                    })
        except Exception as e:
            error_code = e.response.get('Error', {}).get('Code', '') if hasattr(e, 'response') else ''
            if error_code == 'NoSuchLifecycleConfiguration':
                self.findings.append({
                    'severity': 'INFO',
                    'category': 'ç”Ÿå‘½å‘¨æœŸç­–ç•¥',
                    'title': 'æœªé…ç½®ç”Ÿå‘½å‘¨æœŸç­–ç•¥',
                    'details': 'æ•°æ®ä¸ä¼šè¢«è‡ªåŠ¨åˆ é™¤'
                })
            # å…¶ä»–é”™è¯¯é™é»˜å¤„ç†
    
    def _check_cloudtrail_events(self):
        """æ£€æŸ¥ CloudTrail äº‹ä»¶"""
        try:
            start_time = datetime.utcnow() - timedelta(days=90)
            
            # æŸ¥è¯¢ç®¡ç†äº‹ä»¶
            response = self.cloudtrail.lookup_events(
                LookupAttributes=[
                    {'AttributeKey': 'ResourceName', 'AttributeValue': self.bucket_name}
                ],
                StartTime=start_time,
                MaxResults=50
            )
            
            events = response.get('Events', [])
            
            # åˆ†ç±»äº‹ä»¶
            lifecycle_events = []
            policy_events = []
            versioning_events = []
            delete_events = []
            other_events = []
            
            for event in events:
                event_name = event['EventName']
                event_data = {
                    'time': event['EventTime'].strftime('%Y-%m-%d %H:%M:%S'),
                    'event': event_name,
                    'user': event.get('Username', 'N/A'),
                    'source_ip': json.loads(event['CloudTrailEvent']).get('sourceIPAddress', 'N/A')
                }
                
                if 'Lifecycle' in event_name:
                    lifecycle_events.append(event_data)
                elif 'Policy' in event_name:
                    policy_events.append(event_data)
                elif 'Versioning' in event_name:
                    versioning_events.append(event_data)
                elif 'Delete' in event_name:
                    delete_events.append(event_data)
                else:
                    other_events.append(event_data)
            
            # ä¿å­˜æ‰€æœ‰äº‹ä»¶ä¾›æŠ¥å‘Šä½¿ç”¨
            self.cloudtrail_events = {
                'lifecycle': lifecycle_events,
                'policy': policy_events,
                'versioning': versioning_events,
                'delete': delete_events,
                'other': other_events
            }
            
            if lifecycle_events:
                self.findings.append({
                    'severity': 'MEDIUM',
                    'category': 'CloudTrail äº‹ä»¶',
                    'title': f'å‘ç° {len(lifecycle_events)} æ¬¡ç”Ÿå‘½å‘¨æœŸç­–ç•¥å˜æ›´',
                    'details': lifecycle_events
                })
            
            if policy_events:
                self.findings.append({
                    'severity': 'MEDIUM',
                    'category': 'CloudTrail äº‹ä»¶',
                    'title': f'å‘ç° {len(policy_events)} æ¬¡ç­–ç•¥å˜æ›´',
                    'details': policy_events
                })
            
            if versioning_events:
                self.findings.append({
                    'severity': 'MEDIUM',
                    'category': 'CloudTrail äº‹ä»¶',
                    'title': f'å‘ç° {len(versioning_events)} æ¬¡ç‰ˆæœ¬æ§åˆ¶å˜æ›´',
                    'details': versioning_events
                })
            
            if delete_events:
                self.findings.append({
                    'severity': 'HIGH',
                    'category': 'CloudTrail äº‹ä»¶',
                    'title': f'å‘ç° {len(delete_events)} æ¬¡åˆ é™¤ç›¸å…³æ“ä½œ',
                    'details': delete_events
                })
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å…³é”®äº‹ä»¶,æ·»åŠ ä¿¡æ¯è¯´æ˜
            if not (lifecycle_events or policy_events or versioning_events or delete_events):
                if events:
                    self.findings.append({
                        'severity': 'INFO',
                        'category': 'CloudTrail äº‹ä»¶',
                        'title': f'è¿‡å» 90 å¤©æœ‰ {len(events)} ä¸ªç®¡ç†äº‹ä»¶',
                        'details': 'æœªå‘ç°å…³é”®çš„é…ç½®å˜æ›´æˆ–åˆ é™¤æ“ä½œ'
                    })
                else:
                    self.findings.append({
                        'severity': 'INFO',
                        'category': 'CloudTrail äº‹ä»¶',
                        'title': 'è¿‡å» 90 å¤©æ— ç®¡ç†äº‹ä»¶',
                        'details': 'æœªå‘ç°ä»»ä½• bucket çº§åˆ«çš„ç®¡ç†æ“ä½œ'
                    })
                
        except Exception as e:
            self.findings.append({
                'severity': 'INFO',
                'category': 'CloudTrail äº‹ä»¶',
                'title': 'æ— æ³•è·å– CloudTrail äº‹ä»¶',
                'details': str(e)
            })
    
    def _check_bucket_policy(self):
        """æ£€æŸ¥ Bucket ç­–ç•¥"""
        try:
            response = self.s3_client.get_bucket_policy(Bucket=self.bucket_name)
            policy = json.loads(response['Policy'])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…è®¸åˆ é™¤çš„è¯­å¥
            delete_permissions = []
            for statement in policy.get('Statement', []):
                if statement.get('Effect') == 'Allow':
                    actions = statement.get('Action', [])
                    if isinstance(actions, str):
                        actions = [actions]
                    
                    delete_actions = [a for a in actions if 'Delete' in a or a == 's3:*']
                    if delete_actions:
                        delete_permissions.append({
                            'principal': statement.get('Principal', 'N/A'),
                            'actions': delete_actions
                        })
            
            if delete_permissions:
                self.findings.append({
                    'severity': 'MEDIUM',
                    'category': 'Bucket ç­–ç•¥',
                    'title': 'å‘ç°å…è®¸åˆ é™¤æ“ä½œçš„ç­–ç•¥',
                    'details': delete_permissions
                })
                
        except Exception as e:
            # NoSuchBucketPolicy æˆ–å…¶ä»–é”™è¯¯éƒ½é™é»˜å¤„ç†
            pass
    
    def _analyze_costs(self):
        """åˆ†æ S3 æˆæœ¬å˜åŒ–"""
        try:
            end_date = datetime.utcnow().date()
            start_date = end_date - timedelta(days=90)
            
            # è·å–è¯¥åŒºåŸŸ S3 æ¯æ—¥æˆæœ¬ï¼ˆæŒ‰ç”¨é‡ç±»å‹åˆ†ç»„ï¼‰
            response = self.ce_client.get_cost_and_usage(
                TimePeriod={
                    'Start': start_date.strftime('%Y-%m-%d'),
                    'End': end_date.strftime('%Y-%m-%d')
                },
                Granularity='DAILY',
                Metrics=['UnblendedCost'],
                Filter={
                    'Dimensions': {
                        'Key': 'SERVICE',
                        'Values': ['Amazon Simple Storage Service']
                    }
                },
                GroupBy=[
                    {'Type': 'DIMENSION', 'Key': 'USAGE_TYPE'}
                ]
            )
            
            # åˆ†ææˆæœ¬æ•°æ®ï¼Œè¿‡æ»¤å‡ºè¯¥åŒºåŸŸçš„æˆæœ¬
            cost_data = []
            region_prefix = self.region.replace('-', '')
            
            for result in response['ResultsByTime']:
                date = result['TimePeriod']['Start']
                
                # æŒ‰ç”¨é‡ç±»å‹åˆ†ç»„ï¼Œè¿‡æ»¤å‡ºè¯¥åŒºåŸŸçš„æˆæœ¬
                usage_costs = {}
                total_cost = 0
                
                for group in result.get('Groups', []):
                    usage_type = group['Keys'][0]
                    cost = float(group['Metrics']['UnblendedCost']['Amount'])
                    
                    # è¿‡æ»¤å‡ºå½“å‰åŒºåŸŸçš„ç”¨é‡ç±»å‹
                    # ä¾‹å¦‚: USE1-TimedStorage-ByteHrs (åŒ—å¼—å‰å°¼äºš), APN1-TimedStorage-ByteHrs (ä¸œäº¬)
                    if cost > 0:
                        # æå–åŒºåŸŸä»£ç 
                        usage_region = usage_type.split('-')[0] if '-' in usage_type else ''
                        
                        # å¦‚æœåŒºåŸŸåŒ¹é…æˆ–æ˜¯å…¨å±€æœåŠ¡ï¼Œåˆ™è®¡å…¥
                        if (region_prefix.upper() in usage_region.upper() or 
                            usage_region in ['', 'Global', 'Worldwide']):
                            usage_costs[usage_type] = cost
                            total_cost += cost
                
                cost_data.append({
                    'date': date,
                    'total_cost': total_cost,
                    'usage_costs': usage_costs
                })
            
            # åˆ†ææˆæœ¬å˜åŒ–
            if len(cost_data) > 1:
                cost_changes = []
                for i in range(1, len(cost_data)):
                    prev_cost = cost_data[i-1]['total_cost']
                    curr_cost = cost_data[i]['total_cost']
                    
                    if prev_cost > 0:
                        change = curr_cost - prev_cost
                        change_pct = (change / prev_cost * 100)
                        
                        # æˆæœ¬ä¸‹é™è¶…è¿‡ 20% å¯èƒ½è¡¨ç¤ºæ•°æ®åˆ é™¤
                        if change_pct < -20:
                            cost_changes.append({
                                'date': cost_data[i]['date'],
                                'prev_cost': prev_cost,
                                'curr_cost': curr_cost,
                                'change': change,
                                'change_pct': change_pct
                            })
                
                if cost_changes:
                    self.findings.append({
                        'severity': 'HIGH',
                        'category': 'æˆæœ¬å¼‚å¸¸',
                        'title': f'æ£€æµ‹åˆ° {len(cost_changes)} æ¬¡æ˜¾è‘—çš„æˆæœ¬ä¸‹é™',
                        'details': cost_changes
                    })
            
            # ä¿å­˜æˆæœ¬æ•°æ®ä¾›æŠ¥å‘Šä½¿ç”¨
            self.cost_data = cost_data
            
        except Exception as e:
            error_msg = str(e)
            # å¦‚æœæ˜¯æƒé™é—®é¢˜æˆ– Cost Explorer æœªå¯ç”¨ï¼Œé™é»˜å¤„ç†
            if 'AccessDenied' in error_msg or 'not subscribed' in error_msg:
                self.cost_data = []
                self.findings.append({
                    'severity': 'INFO',
                    'category': 'æˆæœ¬åˆ†æ',
                    'title': 'æ— æ³•è·å–æˆæœ¬æ•°æ®',
                    'details': 'éœ€è¦ Cost Explorer æƒé™æˆ–å¯ç”¨ Cost Explorer'
                })
            else:
                self.cost_data = []
                self.findings.append({
                    'severity': 'INFO',
                    'category': 'æˆæœ¬åˆ†æ',
                    'title': 'æ— æ³•è·å–æˆæœ¬æ•°æ®',
                    'details': error_msg
                })
    
    def _analyze_current_objects(self):
        """åˆ†æå½“å‰å¯¹è±¡(ä¼˜åŒ–ç‰ˆ - é™åˆ¶å†…å­˜ä½¿ç”¨)"""
        try:
            print("  æ­£åœ¨åˆ—å‡ºå¯¹è±¡(å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
            paginator = self.s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(
                Bucket=self.bucket_name,
                PaginationConfig={'PageSize': 1000}  # æ¯é¡µ1000ä¸ªå¯¹è±¡
            )
            
            total_objects = 0
            total_size = 0
            prefix_stats = defaultdict(lambda: {'count': 0, 'size': 0})
            
            page_count = 0
            MAX_PREFIXES = 1000  # é™åˆ¶å‰ç¼€æ•°é‡ä»¥èŠ‚çœå†…å­˜
            MAX_OBJECTS = 10000000  # æœ€å¤šå¤„ç†1000ä¸‡ä¸ªå¯¹è±¡
            
            for page in pages:
                page_count += 1
                if page_count % 10 == 0:
                    print(f"  å·²å¤„ç† {total_objects:,} ä¸ªå¯¹è±¡...")
                
                for obj in page.get('Contents', []):
                    total_objects += 1
                    total_size += obj['Size']
                    
                    # æŒ‰å‰ç¼€ç»Ÿè®¡ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                    if len(prefix_stats) < MAX_PREFIXES:
                        prefix = obj['Key'].split('/')[0] if '/' in obj['Key'] else 'root'
                        prefix_stats[prefix]['count'] += 1
                        prefix_stats[prefix]['size'] += obj['Size']
                
                # å¦‚æœå¯¹è±¡æ•°é‡è¿‡å¤šï¼Œæå‰ç»“æŸï¼ˆé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰
                if total_objects >= MAX_OBJECTS:
                    print(f"  å¯¹è±¡æ•°é‡è¿‡å¤šï¼Œåœæ­¢è¯¦ç»†ç»Ÿè®¡...")
                    break
            
            print(f"  å®Œæˆ! å…± {total_objects:,} ä¸ªå¯¹è±¡")
            
            self.current_stats = {
                'total_objects': total_objects,
                'total_size_gb': total_size / (1024**3),
                'prefix_stats': dict(prefix_stats)
            }
            
        except Exception as e:
            self.current_stats = {'error': str(e)}
    
    def _generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("åˆ†æç»“æœæ±‡æ€»")
        print(f"{'='*80}\n")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        high_findings = [f for f in self.findings if f['severity'] == 'HIGH']
        medium_findings = [f for f in self.findings if f['severity'] == 'MEDIUM']
        info_findings = [f for f in self.findings if f['severity'] == 'INFO']
        
        # æ˜¾ç¤ºé«˜å±å‘ç°
        if high_findings:
            print(f"ğŸ”´ é«˜å±å‘ç° ({len(high_findings)} é¡¹):")
            print("-" * 80)
            for finding in high_findings:
                print(f"\n  [{finding['category']}] {finding['title']}")
                self._print_details(finding['details'], indent=4)
        
        # æ˜¾ç¤ºä¸­å±å‘ç°
        if medium_findings:
            print(f"\nğŸŸ¡ ä¸­å±å‘ç° ({len(medium_findings)} é¡¹):")
            print("-" * 80)
            for finding in medium_findings:
                print(f"\n  [{finding['category']}] {finding['title']}")
                self._print_details(finding['details'], indent=4)
        
        # æ˜¾ç¤ºä¿¡æ¯
        if info_findings:
            print(f"\nğŸ”µ ä¿¡æ¯ ({len(info_findings)} é¡¹):")
            print("-" * 80)
            for finding in info_findings:
                print(f"\n  [{finding['category']}] {finding['title']}")
                self._print_details(finding['details'], indent=4)
        
        # å½“å‰çŠ¶æ€
        print(f"\n{'='*80}")
        print("å½“å‰ Bucket çŠ¶æ€")
        print(f"{'='*80}\n")
        if hasattr(self, 'current_stats'):
            if self.current_stats.get('skipped'):
                print("  â­ï¸  è·³è¿‡å¯¹è±¡ç»Ÿè®¡ (ä½¿ç”¨äº† --skip-listing å‚æ•°)")
            elif 'error' not in self.current_stats:
                print(f"  å¯¹è±¡æ€»æ•°: {self.current_stats.get('total_objects', 0):,}")
                print(f"  æ€»å¤§å°: {self.current_stats.get('total_size_gb', 0):.2f} GB")
        
        # ç»“è®ºå’Œå»ºè®®
        print(f"\n{'='*80}")
        print("ç»“è®ºå’Œå»ºè®®")
        print(f"{'='*80}\n")
        
        if high_findings:
            print("  âš ï¸  å‘ç°å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±çš„é—®é¢˜!")
            print("\n  å»ºè®®ç«‹å³é‡‡å–ä»¥ä¸‹æªæ–½:")
            print("  1. æ£€æŸ¥ç”Ÿå‘½å‘¨æœŸç­–ç•¥,ç¡®è®¤æ˜¯å¦ç¬¦åˆé¢„æœŸ")
            print("  2. å®¡æŸ¥ CloudTrail äº‹ä»¶,ç¡®å®šåˆ é™¤æ“ä½œçš„æ¥æº")
            print("  3. å¦‚æœå¯ç”¨äº†ç‰ˆæœ¬æ§åˆ¶,æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤åˆ é™¤çš„å¯¹è±¡")
            print("  4. å¯ç”¨ CloudTrail æ•°æ®äº‹ä»¶å’Œ S3 Server Access Logging")
            print("  5. é…ç½® S3 Inventory ä»¥ä¾¿æœªæ¥è¿½è¸ª")
        else:
            print("  âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®ä¸¢å¤±è¿¹è±¡")
            print("\n  å»ºè®®:")
            print("  1. å¯ç”¨ç‰ˆæœ¬æ§åˆ¶ä»¥é˜²æ­¢æ„å¤–åˆ é™¤")
            print("  2. å¯ç”¨ CloudTrail æ•°æ®äº‹ä»¶ç›‘æ§")
            print("  3. é…ç½® S3 Inventory å®šæœŸç”Ÿæˆå¯¹è±¡æ¸…å•")
        
        # ä¿å­˜æŠ¥å‘Šåˆ° logs ç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
        json_file = os.path.join(self.logs_dir, f"s3-analysis-{self.bucket_name}-{timestamp}.json")
        md_file = os.path.join(self.logs_dir, f"s3-analysis-{self.bucket_name}-{timestamp}.md")
        
        report_data = {
            'bucket': self.bucket_name,
            'analysis_time': datetime.now().isoformat(),
            'findings': self.findings,
            'current_stats': getattr(self, 'current_stats', {}),
            'cloudwatch_data': {
                'size_data': [{'timestamp': d['Timestamp'].isoformat(), 'bytes': d['Average']} 
                             for d in getattr(self, 'size_data', [])],
                'count_data': [{'timestamp': d['Timestamp'].isoformat(), 'count': d['Average']} 
                              for d in getattr(self, 'count_data', [])]
            },
            'cost_data': getattr(self, 'cost_data', [])
        }
        
        # ä¿å­˜ JSON
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        self._generate_markdown_report(md_file, report_data)
        
        print(f"\n  JSON æŠ¥å‘Šå·²ä¿å­˜è‡³: {json_file}")
        print(f"  Markdown æŠ¥å‘Šå·²ä¿å­˜è‡³: {md_file}")
        print(f"\n{'='*80}\n")
    
    def _generate_markdown_report(self, filename, report_data):
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜
            f.write(f"# S3 æ•°æ®ä¸¢å¤±åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**Bucket**: `{report_data['bucket']}`  \n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.fromisoformat(report_data['analysis_time']).strftime('%Y-%m-%d %H:%M:%S')}  \n")
            f.write(f"**åˆ†æå‘¨æœŸ**: è¿‡å» 90 å¤©\n\n")
            f.write("---\n\n")
            
            # æ‰§è¡Œæ‘˜è¦
            f.write("## ğŸ“Š æ‰§è¡Œæ‘˜è¦\n\n")
            high_findings = [f for f in self.findings if f['severity'] == 'HIGH']
            medium_findings = [f for f in self.findings if f['severity'] == 'MEDIUM']
            info_findings = [f for f in self.findings if f['severity'] == 'INFO']
            
            if high_findings:
                f.write(f"ğŸ”´ **é«˜å±å‘ç°**: {len(high_findings)} é¡¹  \n")
            if medium_findings:
                f.write(f"ğŸŸ¡ **ä¸­å±å‘ç°**: {len(medium_findings)} é¡¹  \n")
            if info_findings:
                f.write(f"ğŸ”µ **ä¿¡æ¯**: {len(info_findings)} é¡¹  \n")
            
            if not high_findings and not medium_findings:
                f.write("âœ… **æœªå‘ç°æ˜æ˜¾çš„æ•°æ®ä¸¢å¤±è¿¹è±¡**\n")
            f.write("\n---\n\n")
            
            # CloudWatch æŒ‡æ ‡è¶‹åŠ¿ - åˆå¹¶æ˜¾ç¤º
            f.write("## ğŸ“ˆ CloudWatch æŒ‡æ ‡è¶‹åŠ¿ (è¿‡å» 90 å¤©)\n\n")
            
            size_data = report_data['cloudwatch_data']['size_data']
            count_data = report_data['cloudwatch_data']['count_data']
            
            if size_data and count_data:
                f.write("| æ—¥æœŸ | å­˜å‚¨é‡ (GB) | å­˜å‚¨å˜åŒ– | å¯¹è±¡æ•°é‡ | å¯¹è±¡å˜åŒ– |\n")
                f.write("|------|------------|---------|---------|---------|\n")
                
                # åˆå¹¶ä¸¤ä¸ªæ•°æ®æº
                for i in range(len(size_data)):
                    date = size_data[i]['timestamp'].split('T')[0]
                    size_gb = size_data[i]['bytes'] / (1024**3)
                    count = int(count_data[i]['count']) if i < len(count_data) else 0
                    
                    # è®¡ç®—å­˜å‚¨é‡å˜åŒ–
                    if i > 0:
                        prev_size = size_data[i-1]['bytes'] / (1024**3)
                        size_change = size_gb - prev_size
                        size_change_pct = (size_change / prev_size * 100) if prev_size > 0 else 0
                        
                        if abs(size_change_pct) > 5:
                            size_change_str = f"{size_change:+.2f} GB ({size_change_pct:+.1f}%)"
                            if size_change_pct < -10:
                                size_change_str = f"âš ï¸ {size_change_str}"
                        else:
                            size_change_str = "-"
                    else:
                        size_change_str = "-"
                    
                    # è®¡ç®—å¯¹è±¡æ•°é‡å˜åŒ–
                    if i > 0 and i < len(count_data):
                        prev_count = int(count_data[i-1]['count'])
                        count_change = count - prev_count
                        
                        if abs(count_change) > 10:
                            count_change_str = f"{count_change:+d}"
                            if count_change < -100:
                                count_change_str = f"âš ï¸ {count_change_str}"
                        else:
                            count_change_str = "-"
                    else:
                        count_change_str = "-"
                    
                    f.write(f"| {date} | {size_gb:.2f} | {size_change_str} | {count:,} | {count_change_str} |\n")
                
                f.write("\n")
            
            f.write("---\n\n")
            
            # æˆæœ¬è¶‹åŠ¿åˆ†æ
            cost_data = report_data.get('cost_data', [])
            if cost_data:
                f.write("## ğŸ’° S3 æˆæœ¬è¶‹åŠ¿ (è¿‡å» 90 å¤©)\n\n")
                
                f.write(f"âš ï¸ **æ³¨æ„**: æ­¤æˆæœ¬æ•°æ®ä¸º {self.region} åŒºåŸŸçš„ S3 æ€»æˆæœ¬ï¼Œä¸ä»…é™äºå•ä¸ª bucketã€‚  \n")
                f.write("å¯ä»¥é€šè¿‡æˆæœ¬å˜åŒ–è¶‹åŠ¿é—´æ¥åˆ¤æ–­æ•°æ®å˜åŒ–ã€‚\n\n")
                
                # è®¡ç®—æ€»æˆæœ¬å’Œå¹³å‡æˆæœ¬
                total_cost = sum(d['total_cost'] for d in cost_data)
                avg_cost = total_cost / len(cost_data) if cost_data else 0
                
                f.write(f"**æ€»æˆæœ¬**: ${total_cost:.2f}  \n")
                f.write(f"**å¹³å‡æ¯æ—¥æˆæœ¬**: ${avg_cost:.2f}  \n\n")
                
                # æˆæœ¬è¶‹åŠ¿è¡¨æ ¼
                f.write("### æ¯æ—¥æˆæœ¬æ˜ç»†\n\n")
                f.write("| æ—¥æœŸ | æˆæœ¬ ($) | å˜åŒ– | ä¸»è¦ç”¨é‡ç±»å‹ |\n")
                f.write("|------|---------|------|------------|\n")
                
                for i, data in enumerate(cost_data[-30:]):  # æ˜¾ç¤ºæœ€è¿‘30å¤©
                    date = data['date']
                    cost = data['total_cost']
                    
                    # è®¡ç®—å˜åŒ–
                    if i > 0:
                        prev_cost = cost_data[i-1]['total_cost']
                        if prev_cost > 0:
                            change = cost - prev_cost
                            change_pct = (change / prev_cost * 100)
                            
                            if abs(change_pct) > 5:
                                change_str = f"{change:+.2f} ({change_pct:+.1f}%)"
                                if change_pct < -20:
                                    change_str = f"âš ï¸ {change_str}"
                            else:
                                change_str = "-"
                        else:
                            change_str = "-"
                    else:
                        change_str = "-"
                    
                    # ä¸»è¦ç”¨é‡ç±»å‹
                    usage_costs = data.get('usage_costs', {})
                    if usage_costs:
                        top_usage = max(usage_costs.items(), key=lambda x: x[1])
                        usage_str = f"{top_usage[0][:30]}... (${top_usage[1]:.2f})"
                    else:
                        usage_str = "-"
                    
                    f.write(f"| {date} | ${cost:.2f} | {change_str} | {usage_str} |\n")
                
                f.write("\n*æ˜¾ç¤ºæœ€è¿‘ 30 å¤©æ•°æ®*\n\n")
                
                # ç”¨é‡ç±»å‹æ±‡æ€»
                f.write("### ç”¨é‡ç±»å‹æ±‡æ€»\n\n")
                usage_summary = {}
                for data in cost_data:
                    for usage_type, cost in data.get('usage_costs', {}).items():
                        if usage_type not in usage_summary:
                            usage_summary[usage_type] = 0
                        usage_summary[usage_type] += cost
                
                if usage_summary:
                    sorted_usage = sorted(usage_summary.items(), key=lambda x: x[1], reverse=True)
                    f.write("| ç”¨é‡ç±»å‹ | æ€»æˆæœ¬ ($) | å æ¯” |\n")
                    f.write("|---------|-----------|------|\n")
                    
                    for usage_type, cost in sorted_usage[:10]:
                        pct = (cost / total_cost * 100) if total_cost > 0 else 0
                        f.write(f"| {usage_type} | ${cost:.2f} | {pct:.1f}% |\n")
                    
                    if len(sorted_usage) > 10:
                        f.write(f"\n*æ˜¾ç¤ºå‰ 10 é¡¹ï¼Œå…± {len(sorted_usage)} é¡¹*\n")
                    f.write("\n")
                
                f.write("---\n\n")
            
            # CloudTrail äº‹ä»¶æ±‡æ€»
            if hasattr(self, 'cloudtrail_events'):
                ct_events = self.cloudtrail_events
                total_events = sum(len(v) for v in ct_events.values())
                
                if total_events > 0:
                    f.write("## ğŸ” CloudTrail äº‹ä»¶æ±‡æ€» (è¿‡å» 90 å¤©)\n\n")
                    
                    # ç»Ÿè®¡æ¦‚è§ˆ
                    f.write("### äº‹ä»¶ç»Ÿè®¡\n\n")
                    f.write("| äº‹ä»¶ç±»å‹ | æ•°é‡ |\n")
                    f.write("|---------|------|\n")
                    if ct_events['lifecycle']:
                        f.write(f"| ç”Ÿå‘½å‘¨æœŸç­–ç•¥å˜æ›´ | {len(ct_events['lifecycle'])} |\n")
                    if ct_events['policy']:
                        f.write(f"| Bucket ç­–ç•¥å˜æ›´ | {len(ct_events['policy'])} |\n")
                    if ct_events['versioning']:
                        f.write(f"| ç‰ˆæœ¬æ§åˆ¶å˜æ›´ | {len(ct_events['versioning'])} |\n")
                    if ct_events['delete']:
                        f.write(f"| âš ï¸ åˆ é™¤ç›¸å…³æ“ä½œ | {len(ct_events['delete'])} |\n")
                    if ct_events['other']:
                        f.write(f"| å…¶ä»–ç®¡ç†æ“ä½œ | {len(ct_events['other'])} |\n")
                    f.write(f"| **æ€»è®¡** | **{total_events}** |\n\n")
                    
                    # è¯¦ç»†äº‹ä»¶åˆ—è¡¨
                    f.write("### è¯¦ç»†äº‹ä»¶\n\n")
                    
                    if ct_events['lifecycle']:
                        f.write(f"#### ç”Ÿå‘½å‘¨æœŸç­–ç•¥å˜æ›´ ({len(ct_events['lifecycle'])} æ¬¡)\n\n")
                        f.write("| æ—¶é—´ | æ“ä½œ | ç”¨æˆ· | æº IP |\n")
                        f.write("|------|------|------|-------|\n")
                        for e in ct_events['lifecycle']:
                            f.write(f"| {e['time']} | {e['event']} | {e['user']} | {e['source_ip']} |\n")
                        f.write("\n")
                    
                    if ct_events['policy']:
                        f.write(f"#### Bucket ç­–ç•¥å˜æ›´ ({len(ct_events['policy'])} æ¬¡)\n\n")
                        f.write("| æ—¶é—´ | æ“ä½œ | ç”¨æˆ· | æº IP |\n")
                        f.write("|------|------|------|-------|\n")
                        for e in ct_events['policy']:
                            f.write(f"| {e['time']} | {e['event']} | {e['user']} | {e['source_ip']} |\n")
                        f.write("\n")
                    
                    if ct_events['versioning']:
                        f.write(f"#### ç‰ˆæœ¬æ§åˆ¶å˜æ›´ ({len(ct_events['versioning'])} æ¬¡)\n\n")
                        f.write("| æ—¶é—´ | æ“ä½œ | ç”¨æˆ· | æº IP |\n")
                        f.write("|------|------|------|-------|\n")
                        for e in ct_events['versioning']:
                            f.write(f"| {e['time']} | {e['event']} | {e['user']} | {e['source_ip']} |\n")
                        f.write("\n")
                    
                    if ct_events['delete']:
                        f.write(f"#### âš ï¸ åˆ é™¤ç›¸å…³æ“ä½œ ({len(ct_events['delete'])} æ¬¡)\n\n")
                        f.write("| æ—¶é—´ | æ“ä½œ | ç”¨æˆ· | æº IP |\n")
                        f.write("|------|------|------|-------|\n")
                        for e in ct_events['delete']:
                            f.write(f"| {e['time']} | {e['event']} | {e['user']} | {e['source_ip']} |\n")
                        f.write("\n")
                    
                    if ct_events['other']:
                        f.write(f"#### å…¶ä»–ç®¡ç†æ“ä½œ ({len(ct_events['other'])} æ¬¡)\n\n")
                        f.write("<details>\n<summary>ç‚¹å‡»å±•å¼€</summary>\n\n")
                        f.write("| æ—¶é—´ | æ“ä½œ | ç”¨æˆ· | æº IP |\n")
                        f.write("|------|------|------|-------|\n")
                        for e in ct_events['other'][:20]:
                            f.write(f"| {e['time']} | {e['event']} | {e['user']} | {e['source_ip']} |\n")
                        if len(ct_events['other']) > 20:
                            f.write(f"\n*æ˜¾ç¤ºå‰ 20 é¡¹,å…± {len(ct_events['other'])} é¡¹*\n")
                        f.write("\n</details>\n\n")
                    
                    f.write("---\n\n")
            
            # ç‰ˆæœ¬æ§åˆ¶åˆ†æ
            if hasattr(self, 'version_analysis'):
                va = self.version_analysis
                
                if va['delete_markers'] or va['noncurrent_analysis']:
                    f.write("## ğŸ“‹ ç‰ˆæœ¬æ§åˆ¶åˆ†æ\n\n")
                    
                    if va['delete_markers']:
                        total_dm = va.get('total_delete_markers', len(va['delete_markers']))
                        shown_dm = len(va['delete_markers'])
                        
                        f.write(f"### åˆ é™¤æ ‡è®° (å…± {total_dm} ä¸ª")
                        if shown_dm < total_dm:
                            f.write(f"ï¼Œæ˜¾ç¤ºå‰ {shown_dm} ä¸ª")
                        f.write(")\n\n")
                        f.write("âš ï¸ è¿™äº›å¯¹è±¡è¢«æ ‡è®°ä¸ºåˆ é™¤,ä½†å¯ä»¥æ¢å¤\n\n")
                        f.write("| å¯¹è±¡é”® | åˆ é™¤æ—¶é—´ | ç‰ˆæœ¬ ID |\n")
                        f.write("|------|---------|----------|\n")
                        for dm in va['delete_markers']:
                            f.write(f"| `{dm['key']}` | {dm['last_modified']} | `{dm['version_id'][:16]}...` |\n")
                        f.write("\n")
                        
                        f.write("**æ¢å¤å‘½ä»¤:**\n\n")
                        f.write("```bash\n")
                        f.write("# æ¢å¤å•ä¸ªå¯¹è±¡\n")
                        if va['delete_markers']:
                            sample = va['delete_markers'][0]
                            f.write(f"aws s3api delete-object --bucket {self.bucket_name} --key '{sample['key']}' --version-id {sample['version_id']}\n")
                        f.write("```\n\n")
                    
                    if va['noncurrent_analysis']:
                        total_nc = va.get('total_noncurrent_objects', len(va['noncurrent_analysis']))
                        shown_nc = len(va['noncurrent_analysis'])
                        
                        f.write(f"### éå½“å‰ç‰ˆæœ¬åˆ†æ (å…± {total_nc} ä¸ªå¯¹è±¡")
                        if shown_nc < total_nc:
                            f.write(f"ï¼Œæ˜¾ç¤ºå‰ {shown_nc} ä¸ª")
                        f.write(")\n\n")
                        f.write("ğŸ“„ è¿™äº›å¯¹è±¡æœ‰éå½“å‰ç‰ˆæœ¬,å¯èƒ½åŒ…å«è¢«è¦†ç›–æˆ–åˆ é™¤çš„æ•°æ®\n\n")
                        
                        f.write("| å¯¹è±¡é”® | éå½“å‰ç‰ˆæœ¬æ•° | æœ€è¿‘ä¿®æ”¹æ—¶é—´ | æ€»å¤§å° (MB) |\n")
                        f.write("|------|--------------|-------------|-------------|\n")
                        for item in va['noncurrent_analysis']:
                            size_mb = item['total_size'] / (1024**2)
                            f.write(f"| `{item['key']}` | {item['noncurrent_count']} | {item['latest_noncurrent']} | {size_mb:.2f} |\n")
                        f.write("\n")
                        
                        f.write("**æŸ¥çœ‹å†å²ç‰ˆæœ¬:**\n\n")
                        f.write("```bash\n")
                        if va['noncurrent_analysis']:
                            sample_key = va['noncurrent_analysis'][0]['key']
                            f.write(f"# åˆ—å‡ºå¯¹è±¡çš„æ‰€æœ‰ç‰ˆæœ¬\n")
                            f.write(f"aws s3api list-object-versions --bucket {self.bucket_name} --prefix '{sample_key}'\n\n")
                            f.write(f"# æ¢å¤åˆ°ç‰¹å®šç‰ˆæœ¬\n")
                            f.write(f"aws s3api copy-object --bucket {self.bucket_name} --copy-source {self.bucket_name}/{sample_key}?versionId=VERSION_ID --key '{sample_key}'\n")
                        f.write("```\n\n")
                    
                    f.write("---\n\n")
            
            # è¯¦ç»†å‘ç°
            if high_findings:
                f.write("## ğŸ”´ é«˜å±å‘ç°\n\n")
                for finding in high_findings:
                    f.write(f"### [{finding['category']}] {finding['title']}\n\n")
                    self._write_markdown_details(f, finding['details'])
                    f.write("\n")
            
            if medium_findings:
                f.write("## ğŸŸ¡ ä¸­å±å‘ç°\n\n")
                for finding in medium_findings:
                    f.write(f"### [{finding['category']}] {finding['title']}\n\n")
                    self._write_markdown_details(f, finding['details'])
                    f.write("\n")
            
            if info_findings:
                f.write("## ğŸ”µ ä¿¡æ¯\n\n")
                for finding in info_findings:
                    f.write(f"### [{finding['category']}] {finding['title']}\n\n")
                    self._write_markdown_details(f, finding['details'])
                    f.write("\n")
            
            # å½“å‰çŠ¶æ€
            f.write("---\n\n")
            f.write("## ğŸ“¦ å½“å‰ Bucket çŠ¶æ€\n\n")
            
            stats = report_data.get('current_stats', {})
            if stats.get('skipped'):
                f.write("â­ï¸ **è·³è¿‡å¯¹è±¡ç»Ÿè®¡** (ä½¿ç”¨äº† --skip-listing å‚æ•°)\n\n")
                f.write("æç¤º: å¦‚éœ€è¯¦ç»†çš„å¯¹è±¡ç»Ÿè®¡,è¯·ä¸å¸¦ --skip-listing å‚æ•°é‡æ–°è¿è¡Œ\n\n")
            elif 'error' not in stats:
                f.write(f"- **å¯¹è±¡æ€»æ•°**: {stats.get('total_objects', 0):,}\n")
                f.write(f"- **æ€»å¤§å°**: {stats.get('total_size_gb', 0):.2f} GB\n\n")
                
                prefix_stats = stats.get('prefix_stats', {})
                if prefix_stats:
                    f.write("### æŒ‰å‰ç¼€ç»Ÿè®¡\n\n")
                    f.write("| å‰ç¼€ | å¯¹è±¡æ•° | å¤§å° (GB) |\n")
                    f.write("|------|--------|----------|\n")
                    
                    sorted_prefixes = sorted(prefix_stats.items(), 
                                           key=lambda x: x[1]['size'], 
                                           reverse=True)
                    
                    for prefix, data in sorted_prefixes[:10]:
                        f.write(f"| `{prefix}` | {data['count']:,} | {data['size']/(1024**3):.2f} |\n")
                    
                    if len(sorted_prefixes) > 10:
                        f.write(f"| ... | ... | ... |\n")
                        f.write(f"\n*æ˜¾ç¤ºå‰ 10 ä¸ªæœ€å¤§çš„å‰ç¼€*\n")
                    f.write("\n")
            
            # ç»“è®ºå’Œå»ºè®®
            f.write("---\n\n")
            f.write("## ğŸ’¡ ç»“è®ºå’Œå»ºè®®\n\n")
            
            if high_findings:
                f.write("### âš ï¸ å‘ç°å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±çš„é—®é¢˜!\n\n")
                f.write("**å»ºè®®ç«‹å³é‡‡å–ä»¥ä¸‹æªæ–½:**\n\n")
                f.write("1. æ£€æŸ¥ç”Ÿå‘½å‘¨æœŸç­–ç•¥,ç¡®è®¤æ˜¯å¦ç¬¦åˆé¢„æœŸ\n")
                f.write("2. å®¡æŸ¥ CloudTrail äº‹ä»¶,ç¡®å®šåˆ é™¤æ“ä½œçš„æ¥æº\n")
                f.write("3. å¦‚æœå¯ç”¨äº†ç‰ˆæœ¬æ§åˆ¶,æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤åˆ é™¤çš„å¯¹è±¡\n")
                f.write("4. å¯ç”¨ CloudTrail æ•°æ®äº‹ä»¶å’Œ S3 Server Access Logging\n")
                f.write("5. é…ç½® S3 Inventory ä»¥ä¾¿æœªæ¥è¿½è¸ª\n")
            else:
                f.write("### âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®ä¸¢å¤±è¿¹è±¡\n\n")
                f.write("**é¢„é˜²æªæ–½å»ºè®®:**\n\n")
                f.write("1. å¯ç”¨ç‰ˆæœ¬æ§åˆ¶ä»¥é˜²æ­¢æ„å¤–åˆ é™¤\n")
                f.write("2. å¯ç”¨ CloudTrail æ•°æ®äº‹ä»¶ç›‘æ§\n")
                f.write("3. é…ç½® S3 Inventory å®šæœŸç”Ÿæˆå¯¹è±¡æ¸…å•\n")
                f.write("4. è®¾ç½® CloudWatch å‘Šè­¦ç›‘æ§å­˜å‚¨é‡å’Œå¯¹è±¡æ•°é‡å˜åŒ–\n")
                f.write("5. å®šæœŸå®¡æŸ¥ Bucket ç­–ç•¥å’Œç”Ÿå‘½å‘¨æœŸé…ç½®\n")
            
            f.write("\n---\n\n")
            f.write("## ğŸ“š å‚è€ƒæ–‡æ¡£\n\n")
            f.write("- [S3 CloudWatch Metrics](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html)\n")
            f.write("- [S3 Versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html)\n")
            f.write("- [S3 Lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)\n")
            f.write("- [CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html)\n")
            f.write("- [S3 Inventory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html)\n")
    
    def _write_markdown_details(self, f, details):
        """å†™å…¥ Markdown æ ¼å¼çš„è¯¦ç»†ä¿¡æ¯"""
        if isinstance(details, str):
            f.write(f"{details}\n\n")
        elif isinstance(details, dict):
            if 'message' in details:
                f.write(f"{details['message']}\n\n")
            
            for key, value in details.items():
                if key == 'message':
                    continue
                    
                if isinstance(value, list) and value:
                    f.write(f"**{key}:**\n\n")
                    
                    if isinstance(value[0], dict):
                        # è¡¨æ ¼æ ¼å¼
                        if len(value) > 0:
                            keys = list(value[0].keys())
                            f.write("| " + " | ".join(keys) + " |\n")
                            f.write("|" + "------|" * len(keys) + "\n")
                            
                            for item in value[:10]:
                                row = []
                                for k in keys:
                                    v = item.get(k, '')
                                    if isinstance(v, dict):
                                        v = json.dumps(v, ensure_ascii=False)
                                    row.append(str(v))
                                f.write("| " + " | ".join(row) + " |\n")
                            
                            if len(value) > 10:
                                f.write(f"\n*æ˜¾ç¤ºå‰ 10 é¡¹,å…± {len(value)} é¡¹*\n")
                    else:
                        for item in value[:10]:
                            f.write(f"- {item}\n")
                        if len(value) > 10:
                            f.write(f"\n*æ˜¾ç¤ºå‰ 10 é¡¹,å…± {len(value)} é¡¹*\n")
                    f.write("\n")
                elif isinstance(value, dict):
                    f.write(f"**{key}:**\n\n```json\n{json.dumps(value, indent=2, ensure_ascii=False)}\n```\n\n")
                else:
                    f.write(f"- **{key}**: {value}\n")
        elif isinstance(details, list):
            for item in details[:10]:
                if isinstance(item, dict):
                    for k, v in item.items():
                        f.write(f"- **{k}**: {v}\n")
                    f.write("\n")
                else:
                    f.write(f"- {item}\n")
            if len(details) > 10:
                f.write(f"\n*æ˜¾ç¤ºå‰ 10 é¡¹,å…± {len(details)} é¡¹*\n")
    
    def _print_details(self, details, indent=2):
        """æ‰“å°è¯¦ç»†ä¿¡æ¯"""
        prefix = " " * indent
        if isinstance(details, dict):
            for key, value in details.items():
                if isinstance(value, (list, dict)):
                    print(f"{prefix}{key}:")
                    self._print_details(value, indent + 2)
                else:
                    print(f"{prefix}{key}: {value}")
        elif isinstance(details, list):
            for item in details[:5]:  # åªæ˜¾ç¤ºå‰ 5 é¡¹
                if isinstance(item, dict):
                    for key, value in item.items():
                        print(f"{prefix}{key}: {value}")
                    print()
                else:
                    print(f"{prefix}- {item}")
            if len(details) > 5:
                print(f"{prefix}... (è¿˜æœ‰ {len(details) - 5} é¡¹)")
        else:
            print(f"{prefix}{details}")


def main():
    parser = argparse.ArgumentParser(
        description='S3 æ•°æ®ä¸¢å¤±åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python s3_deletion_analyzer.py --bucket veeam-backup-bucket
  
  # æŒ‡å®šåŒºåŸŸ
  python s3_deletion_analyzer.py --bucket my-bucket --region us-west-2
  
  # è·³è¿‡å¯¹è±¡åˆ—è¡¨(é€‚ç”¨äºå¤§å‹ bucket)
  python s3_deletion_analyzer.py --bucket large-bucket --skip-listing
  
æ³¨æ„:
  - æŠ¥å‘Šå°†ä¿å­˜åœ¨å½“å‰ç›®å½•çš„ logs/ å­ç›®å½•ä¸‹
  - å¯¹äºåŒ…å«æ•°ç™¾ä¸‡å¯¹è±¡çš„ bucket,å»ºè®®ä½¿ç”¨ --skip-listing å‚æ•°
        """
    )
    parser.add_argument('--bucket', required=True, help='S3 bucket åç§°')
    parser.add_argument('--region', default='us-east-1', help='AWS åŒºåŸŸ (é»˜è®¤: us-east-1)')
    parser.add_argument('--skip-listing', action='store_true', 
                       help='è·³è¿‡å¯¹è±¡åˆ—è¡¨ç»Ÿè®¡(é€‚ç”¨äºå¤§å‹ bucket,å¯æ˜¾è‘—åŠ å¿«åˆ†æé€Ÿåº¦)')
    
    args = parser.parse_args()
    
    try:
        analyzer = S3DeletionAnalyzer(args.bucket, args.region, args.skip_listing)
        analyzer.analyze()
    except Exception as e:
        print(f"\né”™è¯¯: {str(e)}\n")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())
